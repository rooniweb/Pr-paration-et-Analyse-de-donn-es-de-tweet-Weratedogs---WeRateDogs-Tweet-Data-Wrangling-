{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Wrangle Report </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Les données du monde réel sont rarement propres et en tant qu'analyste de données, c'est notre travail de rassembler, d'évaluer\n",
    "et nettoyer les données pour les rendre viables et praticable pour l'analyse.\n",
    "L'ensemble de données sur lequel j'aitravaillé est une archive Twitter de l'utilisateur **@dog_rates**. WeRateDogs est un compte Twitter qui évalue les chiens des gens avec un commentaire humoristique sur le chien. Le compte a été lancé en **2015** par l'étudiant **Matt Nelson** et a attiré l'attention des médias internationaux. Il partage des images de chiens et rédige un bref panégyrique sur le chien, puis laisse ses abonnés le noter en le mettant en favori. En demandant à WeRateDogs de partager avec nous certains de leurs tweets, ils l'ont fait. Ils ont partagé plus de 5000 de leurs tweets qui contiennent des données de base. Parfois, dans leur bref panégyrique, ils mentionnent la race du chien, et d'autres noms. Mais grâce à Udacity, ils ont effectué des procédures pour classer les chiens en fonction de leurs images partagées avec les tweets.\n",
    "Leur archive est fournie par udacity, nous avons téléchargé le fichier image_prediction à partir du serveur hébergé Udacity par programmation et téléchargé des données supplémentaires à l'aide de tweepy pour collecter les données de l'API tweeter .\n",
    "Après avoir collecté les données, nous devons les évaluer et trouver des problèmes de qualité et d'ordre avant de les nettoyer, puis l'analyser.\n",
    "\n",
    "\n",
    "#### 1. Collecte de données \n",
    "* `Twitter_archive_enhanced.csv` a été fourni par Udacity et a été chargé dans une base de données appelée `archive_df`.\n",
    "* `Image_predictions.tsv` a été téléchargé par programmation à l'aide de la biliothèque **Requests** et a été chargé dans une base de données appelée `pred_df`.\n",
    "* Des données supplémentaires ont été recueillies à l'aide de l'**API Tweepy** et stockées dans un fichier 'tweet_json.txt' et une trame de données appelée `tweet_df`.\n",
    "* Dans une volonté d'élargir le travail, j'ai recueilli par voie de webscraping, des informations complètes sur les langues des tweet sur Wikipédia, qui sont ensuite reconduites dans une base de données appelée `lang_df`, à l'aide de **beautifulsoup**.\n",
    "\n",
    "#### 2. Évaluation des données \n",
    "Évaluer les données consisterait à identifier sur les données les problèmes de qualités et d'ordres (structures).\n",
    "> ##### Problèmes de qualité\n",
    "> Données `archive_df`\n",
    "> 1. Le type de la variable `timestamp` n'est pas adéquat pour déterminer le temps.\n",
    "> 2. Nos trois variables `retweeted_status_*` et deux variables `in_reply_to_*` sont quasis vides et donc doivent être écartés de l'étude.\n",
    "> 3. `expanded_urls` contient aussi d'importantes valeurs manquantes mais pas autant que les dernières variables citées.\n",
    "> 4. Notation pas du tout régulière, donc  remise en cause. \n",
    "> 5. Dans la variable `text` on découvre une autre variable cachée qu'est le `lien`.\n",
    "> 6. La variable `source` affiche plus que la vraie source essentiel qu'est l'appareil ou le moyen d'acces de Twitter.\n",
    "> 7. On voit des noms de chiens incorrects comme *a* par exemple répété 55 fois et *an* 7 fois dans le jeu de donnée. <br>\n",
    "> 8. Les valeurs supposées vides sont remplies par la valeur 'None' au lieu de `None`.\n",
    "> Données  `pred_df`\n",
    "> 9. Pas de valeurs manquantes heureusement, mais nous avons 3 prédictions pour une seule image, faudra t-il chosir uniquement donc celle qui a la meilleure prédiction puisque nous ne sommes pas dans une étude d'apprentissage automatique.\n",
    "> 10. Le noms des races des chiens doit être redéfini.<br>\n",
    "> Données  `tweet_df`\n",
    "> 11. Ce dataset a énormément de valeurs manquantes, contient très probablement des variables inutiles à notre étude et certaines variables ont des valeurs uniques. \n",
    "> 12. Le nom des langues dans la variable `lang` a été abrégée, elle doit être explicite !\n",
    "\n",
    "> ##### Problèmes d'ordre\n",
    "> 1.  Les variables `rating_numerator` et `rating_denominator` peuvent faire une seule variable au lieu de deux puisqu'il s'agit d'une note, même si on est en façe d'une notation assez particulière.\n",
    "> 2. Dénomination mal géré pour les étapes des chiens.\n",
    "> 3. Les 3 premiers datasets ne doivent former qu'un seul pour former une seul unité d'observation qui axée sur l'analyse des tweets sur les chiens. Cette étape sera effectué un plus tard que les deux premières. On sous entend qu'on aura déjà récupéré le nom complet de la langue des tweets auprès de la 4ième base externes (`lang_df`). \n",
    "\n",
    "> #### 3. Nettoyer le données\n",
    "> 1. Toujours faire d'abord une copie des données d'origine avant le nettoyage. Le nettoyage comprend la fusion de données individuelles selon les règles de l'ordre des données. Le résultat est un maître Pandas DataFrame (ou DataFrames, le cas échéant) de haute qualité et bien rangé sous le nom de `twitter_archive_master.csv`.\n",
    "> 2. Regrouper les colonnes (doggo, floofer, pupper, puppo) concernant les étapes des chiens pour créer une seule colonne stade_chien.\n",
    "> 3. La variable timestamp converti en pandas datetime.\n",
    "> 4. Suppression de tous les variables qui sont pratiquement vides comme retweeted_status_* et deux variables in_reply_to_*.\n",
    "> 5. Rédéfinition le système de notation des tweets tout en concervant les écarts (normalisation) et créer une seule variable > single_note (rating_numerator/rating_denominator) au lieu de deux.\n",
    "> 6. Suppression les liens contenus dans la variable text.\n",
    "> 7. Utilisation de regex pour extraire la vraie source des tweets.\n",
    "> 8. Remplacement les valeurs 'None' par NaN ainsi que les noms de chiens invalides.\n",
    "> 9. Mise en majuscule toutes les petites lettres de départ sur les noms des races de chien.\n",
    "> 10. Obtention du nom complet des langues utilisées pour les tweets par jointure interne les bases `tweet_df_copy` et `lang_df_copy` les copies respectives de tweet_df et de lang_df.\n",
    "> 11. Remplacement de toutes les None value par des numpy nan et enfin suppression de les toutes variables jugées inutiles à l'analyse des données et pour la suite.\n",
    "> 12. Joindre les 3 premiers datasets pour former qu'une seule unité d'observation et supprimer les colonnes doublons post-jointure.\n",
    "\n",
    "#### 4. Stocker les données\n",
    "Enregistrement de la trame de données nettoyé (principal) en lieu sûr dans un fichier csv nomé `twitter_archive_master.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
